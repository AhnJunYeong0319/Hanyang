{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“package ‘ISLR’ was built under R version 4.0.2”\n"
     ]
    }
   ],
   "source": [
    "# use the Default data to classify default with income+balance+student\n",
    "library(ISLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"I need to ensure that the splits contain all relevant categories.\"\n",
      "[1] \"For \\\"Default\\\" the prof. said random sampling is OK since \\\"defaulted\\\" is infrequent, but not too infrequent.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     \n",
       "        No  Yes\n",
       "  No  6850 2817\n",
       "  Yes  206  127"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split the data in tain and test with one random sample of 80/20\n",
    "# what do you have to consider here?\n",
    "\n",
    "print('I need to ensure that the splits contain all relevant categories, for example by checking the count with table().')\n",
    "\n",
    "table(Default$default, Default$student)\n",
    "\n",
    "print('For \"Default\" the prof. said random sampling is OK since \"defaulted\" is infrequent, but not too infrequent.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample 20% for test set\n",
    "testindices <- sample(nrow(Default), nrow(Default)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new data frame objects for train and test\n",
    "train.data <- Default[-testindices,]\n",
    "test.data <- Default[testindices,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify this split with\n",
    "    logistic regression\n",
    "    linear discriminant analysis\n",
    "    quadratic discriminant analysis: qda() from MASS\n",
    "    k-nearest neighbors: knn() from class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression fit\n",
    "log.fit <- glm(default ~ income+balance+student, family='binomial', train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA fit\n",
    "library(MASS)\n",
    "lda.fit <- lda(default ~ income+balance+student, train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QDA fit\n",
    "qda.fit <- qda(default ~ income+balance+student, train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-nearest fit: different approach: does not build a model, but directly classifies new data\n",
    "# so this is actually already the prediction\n",
    "# I use cbind() to provide the predictors, since knn does not provide the \"equation\" form (y~x)\n",
    "library(class)\n",
    "knn.fit <- knn(cbind(as.integer(train.data$student), train.data[,3:4]), cbind(as.integer(test.data$student), test.data[,3:4]), train.data$default, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = default ~ income + balance + student, family = \"binomial\", \n",
       "    data = train.data)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.4042  -0.1510  -0.0593  -0.0221   3.6295  \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept) -1.037e+01  5.264e-01 -19.709  < 2e-16 ***\n",
       "income      -4.060e-06  8.988e-06  -0.452  0.65148    \n",
       "balance      5.609e-03  2.500e-04  22.435  < 2e-16 ***\n",
       "studentYes  -7.451e-01  2.571e-01  -2.898  0.00375 ** \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 2380.8  on 7999  degrees of freedom\n",
       "Residual deviance: 1304.3  on 7996  degrees of freedom\n",
       "AIC: 1312.3\n",
       "\n",
       "Number of Fisher Scoring iterations: 8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(log.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "discuss the relevance of predictors\n",
    "create a confusion matrix\n",
    "calculate precision, recall and F1-score\n",
    "discuss differences between the approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####### log model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"income is the only slope to be NOT considered different from 0 in this model, so the others seem to be the relevant predictors\"\n",
      "[1] \"\"\n",
      "     prediction\n",
      "true    No  Yes\n",
      "  No  1935    5\n",
      "  Yes   39   21\n",
      "[1] \"\"\n",
      "[1] \"0.807692307692308 0.35 0.488372093023256\"\n"
     ]
    }
   ],
   "source": [
    "# discuss the relevance of predictors\n",
    "\n",
    "# summary(log.fit)\n",
    "\n",
    "print('income is the only slope to be NOT considered different from 0 in this model, so the others seem to be the relevant predictors')\n",
    "\n",
    "# create a confusion matrix\n",
    "log.pred <- rep('No', nrow(test.data))\n",
    "log.pred[predict(log.fit, test.data, type='response')>=.5] = 'Yes'\n",
    "\n",
    "log.matrix <- table(true=test.data$default, prediction=log.pred)\n",
    "print('')\n",
    "print(log.matrix)\n",
    "\n",
    "\n",
    "# calculate precision, recall and F1-score\n",
    "precision = log.matrix[2,2]/sum(log.matrix[,2])\n",
    "recall = log.matrix[2,2]/sum(log.matrix[2,])\n",
    "F1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print('')\n",
    "print(paste(precision, recall, F1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "lda model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"normalizing of numerical data can be used to infer predictor strength from model coefficients\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "lda(default ~ scale(income) + scale(balance) + student, data = train.data)\n",
       "\n",
       "Prior probabilities of groups:\n",
       "      No      Yes \n",
       "0.965875 0.034125 \n",
       "\n",
       "Group means:\n",
       "    scale(income) scale(balance) studentYes\n",
       "No    0.004302731     -0.0656359  0.2900220\n",
       "Yes  -0.121784621      1.8577605  0.3736264\n",
       "\n",
       "Coefficients of linear discriminants:\n",
       "                        LD1\n",
       "scale(income)   0.009580553\n",
       "scale(balance)  1.083582645\n",
       "studentYes     -0.223066817"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"balance is by far the strongest predictor, income looks irrelevant\"\n",
      "[1] \"\"\n",
      "     prediction\n",
      "true    No  Yes\n",
      "  No  1938    2\n",
      "  Yes   47   13\n",
      "[1] \"\"\n",
      "[1] \"0.866666666666667 0.216666666666667 0.346666666666667\"\n"
     ]
    }
   ],
   "source": [
    "# discuss the relevance of predictors\n",
    "\n",
    "print('normalizing of numerical data can be used to infer predictor strength from model coefficients')\n",
    "lda.norm.fit <- lda(default ~ scale(income)+scale(balance)+student, train.data)\n",
    "lda.norm.fit\n",
    "\n",
    "print('balance is by far the strongest predictor, income looks irrelevant')\n",
    "\n",
    "# create a confusion matrix\n",
    "lda.matrix <- table(true=test.data$default, prediction=predict(lda.fit, test.data)$class)\n",
    "print('')\n",
    "print(lda.matrix)\n",
    "\n",
    "# calculate precision, recall and F1-score\n",
    "precision = lda.matrix[2,2]/sum(lda.matrix[,2])\n",
    "recall = lda.matrix[2,2]/sum(lda.matrix[2,])\n",
    "F1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print('')\n",
    "print(paste(precision, recall, F1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qda model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"we have not learned how to deal with this\"\n",
      "[1] \"\"\n",
      "     prediction\n",
      "true    No  Yes\n",
      "  No  1936    4\n",
      "  Yes   43   17\n",
      "[1] \"\"\n",
      "[1] \"0.80952380952381 0.283333333333333 0.419753086419753\"\n"
     ]
    }
   ],
   "source": [
    "# discuss the relevance of predictors\n",
    "\n",
    "print('we have not learned how to deal with this')\n",
    "\n",
    "# create a confusion matrix\n",
    "qda.matrix <- table(true=test.data$default, prediction=predict(qda.fit, test.data)$class)\n",
    "print('')\n",
    "print(qda.matrix)\n",
    "\n",
    "# calculate precision, recall and F1-score\n",
    "precision = qda.matrix[2,2]/sum(qda.matrix[,2])\n",
    "recall = qda.matrix[2,2]/sum(qda.matrix[2,])\n",
    "F1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print('')\n",
    "print(paste(precision, recall, F1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"in knn, all predictors are equally relevant by default weighted similarly to calculate distances\"\n",
      "[1] \"\"\n",
      "     predicted\n",
      "true    No  Yes\n",
      "  No  1905   35\n",
      "  Yes   36   24\n",
      "[1] \"\"\n",
      "[1] \"0.406779661016949 0.4 0.403361344537815\"\n"
     ]
    }
   ],
   "source": [
    "# discuss the relevance of predictors\n",
    "\n",
    "print('in knn, all predictors are equally relevant by default weighted similarly to calculate distances')\n",
    "\n",
    "# create a confusion matrix\n",
    "knn.matrix <- table(true=test.data$default, predicted=knn.fit)\n",
    "print('')\n",
    "print(knn.matrix)\n",
    "\n",
    "# calculate precision, recall and F1-score\n",
    "precision = knn.matrix[2,2]/sum(knn.matrix[,2])\n",
    "recall = knn.matrix[2,2]/sum(knn.matrix[2,])\n",
    "F1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print('')\n",
    "print(paste(precision, recall, F1))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use logistic with 10-fold CV, since it has the highest F1 for test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.021404846941918"
      ],
      "text/latex": [
       "0.021404846941918"
      ],
      "text/markdown": [
       "0.021404846941918"
      ],
      "text/plain": [
       "[1] 0.02140485"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(boot)\n",
    "cv.fit <- cv.glm(Default, glm(default ~ income+balance+student, family='binomial', Default), K=10)\n",
    "\n",
    "# prediction error\n",
    "cv.fit$delta[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.022"
      ],
      "text/latex": [
       "0.022"
      ],
      "text/markdown": [
       "0.022"
      ],
      "text/plain": [
       "[1] 0.022"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"CV has minimally lower error than single split. You could also have compared results based on confusion matrix or F1\"\n"
     ]
    }
   ],
   "source": [
    "# compare to error of single model\n",
    "mean(test.data$default!=log.pred)\n",
    "\n",
    "print('CV has minimally lower error than single split. You could also have compared results based on confusion matrix or F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points:\n",
    "\n",
    "- 10 statement sample/split \n",
    "- 05/20 for each model\n",
    "- 05/20 for each predictor statement\n",
    "- 05/20 for each confusion matrix and precision/recall/F1\n",
    "- 10 eval approaches\n",
    "- 10 for CV\n",
    "- 10 for interpretation\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
