{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:gray'> JunYeong Ahn / 2020047029 / Assignment2 </span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background : yellow'> • use the Default data to classify default with income+balance+student </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Here, I start with loading the essential packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ISLR)\n",
    "library(MASS)\n",
    "library(dplyr)\n",
    "library(class)\n",
    "library(boot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " default    student       balance           income     \n",
       " No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n",
       " Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n",
       "                       Median : 823.6   Median :34553  \n",
       "                       Mean   : 835.4   Mean   :33517  \n",
       "                       3rd Qu.:1166.3   3rd Qu.:43808  \n",
       "                       Max.   :2654.3   Max.   :73554  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(Default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background:yellow'> • split the data in tain and test with one random sample of 80/20 </span>\n",
    "#### <span style='background:yellow'> • ? what do you have to consider here? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-To split the original 'default' data into a train set and a test set, we should use sample() function who takes a sample of the specified size from the elements of x using either with or without replacement. Here, when I make a train set whose size is four-fifths the number of obsevation of Default without replacement, the rest of it naturally becomes a test set. Strictly speaking, 'train' is a collection of indices, not of direct values. <br>\n",
    "If we do not draw the sample randomly, there's a chance that the sample becomes to be biased one, which also could lead to 'underfitting'. In addition, not to overfit or underfit, we should properly set the balance on the ratio of sample size to test size - here, 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(38)\n",
    "train = sample(nrow(Default),size = nrow(Default)/(5/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "-The Logistic Regression is used to model the probability of a class Default / Not default. Each object being detected in the data would be assigned a probability between 0 and 1, with a sum of one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-To fit the Logistic Regression model using our train set, I used glm() function who takes default for response and income & balance & student for predictors to take those predictors into consideration at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm.fit <- glm(default ~ income + balance + student, data = Default, subset = train, family = \"binomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background:yellow'> discuss the relevance of predictors </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Using sunmmary() function, I tried to get the overall picture of my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = default ~ income + balance + student, family = \"binomial\", \n",
       "    data = Default, subset = train)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.0956  -0.1451  -0.0583  -0.0215   3.7045  \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept) -1.076e+01  5.501e-01 -19.561  < 2e-16 ***\n",
       "income       4.457e-06  9.119e-06   0.489  0.62501    \n",
       "balance      5.661e-03  2.602e-04  21.761  < 2e-16 ***\n",
       "studentYes  -7.173e-01  2.615e-01  -2.743  0.00609 ** \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 2272.8  on 7999  degrees of freedom\n",
       "Residual deviance: 1255.9  on 7996  degrees of freedom\n",
       "AIC: 1263.9\n",
       "\n",
       "Number of Fisher Scoring iterations: 8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(glm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Seeing this, especially 'Coefficients' part, we can realize that the p-value of ***income*** predictor is conspicuously larger than\n",
    "that of other predictors (and also ***income***'s p-value alone is greater than 0.05, which generally is the standard when deciding whether reject\n",
    "the Null hypothesis or not). <br> To make it much sure before judging (by my eyes) that 'income' is trash as a predictor, I'll use step() function who chooses better model in a stepwise algorithm to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:  AIC=1263.9\n",
      "default ~ income + balance + student\n",
      "\n",
      "          Df Deviance    AIC\n",
      "- income   1   1256.1 1262.1\n",
      "<none>         1255.9 1263.9\n",
      "- student  1   1263.3 1269.3\n",
      "- balance  1   2264.3 2270.3\n",
      "\n",
      "Step:  AIC=1262.14\n",
      "default ~ balance + student\n",
      "\n",
      "          Df Deviance    AIC\n",
      "<none>         1256.1 1262.1\n",
      "- student  1   1281.2 1285.2\n",
      "- balance  1   2265.0 2269.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:  glm(formula = default ~ balance + student, family = \"binomial\", \n",
       "    data = Default, subset = train)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)      balance   studentYes  \n",
       " -10.581678     0.005661    -0.814955  \n",
       "\n",
       "Degrees of Freedom: 7999 Total (i.e. Null);  7997 Residual\n",
       "Null Deviance:\t    2273 \n",
       "Residual Deviance: 1256 \tAIC: 1262"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step(glm.fit, direction = \"backward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-As the code above shows, when using step() function ***income*** was discarded, but ***balance*** and ***student*** have survived to be meaningful. <br>\n",
    "Seeing the 'Coefficients' part, it turns out that ***balance*** has a positive effect (relationship) on ***default*** and ***student(Yes)*** has a negative one on ***default***. <br><br>\n",
    "In a conclusion, although we've used and are going to use three predictors in our model, this doesn't mean that they are all of the importance; ***income*** and ***student*** predictors are necessary and important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:blue'>※ For the instructions <span style='background:yellow'> create a confusion matrix </span> and <span style='background:yellow'> calculate precision, recall and F1-score </span>, I repeat almost the same tasks three times more.\n",
    "     Therefore, maybe there would be no further trivial explanations, of course except for special cases. ※ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background:yellow'> create a confusion matrix </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Before creating a confusion matrix, I predicted the response against the data Default[-train,], which corresponds to the test set;\n",
    "\n",
    "Default[train,] = train set / Default[-train,] = test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm.probs=predict(glm.fit, Default[-train,],type=\"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Just checking what number No / Yes was converted into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Yes</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>No</th><td>0</td></tr>\n",
       "\t<tr><th scope=row>Yes</th><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "  & Yes\\\\\n",
       "\\hline\n",
       "\tNo & 0\\\\\n",
       "\tYes & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Yes |\n",
       "|---|---|\n",
       "| No | 0 |\n",
       "| Yes | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "    Yes\n",
       "No  0  \n",
       "Yes 1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "contrasts(Default$default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Our test set has 2000 observations, so I made 'glm.pred', full of \"No\", whose length is 2000 too and change the value from \"No\" to \"Yes\" \n",
    "when the value of the prediction we made is higher than 0.5 (the middle point of 0(\"No\") and 1(\"Yes\"))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm.pred=rep(\"No\", 2000)\n",
    "glm.pred[glm.probs>0.5]=\"Yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Confusion matrix is a specific table layout that allows visualization of the performance of an algorithm. Here, this confusion matrix shows us four cases (2*2) represented by \"Yes\" or \"No\" of our prediction and true test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     true\n",
       "pred    No  Yes\n",
       "  No  1910   49\n",
       "  Yes   14   27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusionmatrix_glm <-table(data.frame(pred = glm.pred, true = Default[-train,]$default))\n",
    "confusionmatrix_glm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background:yellow'> calculate precision, recall and F1-score </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision (the fraction of relevant instances among the retrieved instances) = True Positive / (True Positive + False Positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.658536585365854"
      ],
      "text/latex": [
       "0.658536585365854"
      ],
      "text/markdown": [
       "0.658536585365854"
      ],
      "text/plain": [
       "[1] 0.6585366"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision_glm <- confusionmatrix_glm[2,2] / sum(confusionmatrix_glm[2,])\n",
    "precision_glm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall (the fraction of the total amount of relevant instances that were actually retrieved) = True Positive / (True Positive + False Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.355263157894737"
      ],
      "text/latex": [
       "0.355263157894737"
      ],
      "text/markdown": [
       "0.355263157894737"
      ],
      "text/plain": [
       "[1] 0.3552632"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recall_glm <- confusionmatrix_glm[2,2] / sum(confusionmatrix_glm[,2])\n",
    "recall_glm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-score is a measure of a test's accuracy, which is calculated from the precision and recall of the test through the harmonic mean of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.461538461538462"
      ],
      "text/latex": [
       "0.461538461538462"
      ],
      "text/markdown": [
       "0.461538461538462"
      ],
      "text/plain": [
       "[1] 0.4615385"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F1score_glm <- 2*precision_glm*recall_glm/(precision_glm+recall_glm)\n",
    "F1score_glm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "-LDA (Linear Discriminant Analysis) is a model that creates a decision boundary by learning the distribution of data and classifies data. This assumes that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes theorem in order to perform prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-To fit the LDA model using our train set, I used lda() function who takes default for response and income & balance & student for predictors to take those predictors into consideration at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit <- lda(default ~ income + balance + student, data = Default, subset = train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background:yellow'> discuss the relevance of predictors </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "lda(default ~ income + balance + student, data = Default, subset = train)\n",
       "\n",
       "Prior probabilities of groups:\n",
       "      No      Yes \n",
       "0.967875 0.032125 \n",
       "\n",
       "Group means:\n",
       "      income   balance studentYes\n",
       "No  33542.64  802.3658  0.2908433\n",
       "Yes 32162.61 1728.7048  0.3735409\n",
       "\n",
       "Coefficients of linear discriminants:\n",
       "                     LD1\n",
       "income      3.370464e-06\n",
       "balance     2.243630e-03\n",
       "studentYes -1.957734e-01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lda.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no way to do this task based on my current knowledge. <br>\n",
    "1.) Simply using the summary() function to check p-value of each predictor by eyes doesn't work here. <br>\n",
    "2.) Step() function also doesn't work well for LDA&QDA ; the code '?step' says that this function is mainly for lm() and glm(). <br>\n",
    "3.) We've briefly learned about how to analyze our model through ANOVA which uses ***categorical independent variables*** and ***a continuous dependent variable***, but here our LDA&QDA model has ***continuous independent variables*** and ***a categorical dependent variable***. <br> <br>\n",
    "So there might be a simple solution fuction for this task, but it is out of my knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background:yellow'> create a confusion matrix </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Predict the response against the data Default[-train,], which corresponds to the test set;\n",
    "\n",
    "Default[train,] = train set / Default[-train,] = test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.pred <- predict(lda.fit, Default[-train,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Confusion matrix for my LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     true\n",
       "pred    No  Yes\n",
       "  No  1916   54\n",
       "  Yes    8   22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusionmatrix_lda <- table(data.frame(pred=lda.pred$class, true=Default[-train,]$default))\n",
    "confusionmatrix_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background:yellow'> calculate precision, recall and F1-score </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.733333333333333"
      ],
      "text/latex": [
       "0.733333333333333"
      ],
      "text/markdown": [
       "0.733333333333333"
      ],
      "text/plain": [
       "[1] 0.7333333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision_lda <- confusionmatrix_lda[2,2] / sum(table(data.frame(lda.pred$class, Default[-train,]$default))[2,])\n",
    "precision_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.289473684210526"
      ],
      "text/latex": [
       "0.289473684210526"
      ],
      "text/markdown": [
       "0.289473684210526"
      ],
      "text/plain": [
       "[1] 0.2894737"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recall_lda <- confusionmatrix_lda[2,2] / sum(table(data.frame(lda.pred$class, Default[-train,]$default))[,2])\n",
    "recall_lda \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.415094339622642"
      ],
      "text/latex": [
       "0.415094339622642"
      ],
      "text/markdown": [
       "0.415094339622642"
      ],
      "text/plain": [
       "[1] 0.4150943"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F1score_lda <- 2*precision_lda*recall_lda/(precision_lda+recall_lda)\n",
    "F1score_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QDA\n",
    "-QDA (Quadratic Discriminant Analysis) results from assuming that the observations from each class are drawn from a Gaussian distribution and that each class has its own covariance matrix, and plugging estimates for the parameters into Bayes theorem in order to perform prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-To fit the QDA model using our train set, I used qda() function who takes default for response and income & balance & student for predictors to take those predictors into consideration at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda.fit <- qda(default ~ income + balance + student, data = Default, subset = train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background:yellow'> discuss the relevance of predictors </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-There's no way I can check this for the same reason with what I wrote about this part in LDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background:yellow'> create a confusion matrix <span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Predict the response against the data Default[-train,], which corresponds to the test set;\n",
    "\n",
    "Default[train,] = train set / Default[-train,] = test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda.pred <- predict(qda.fit, Default[-train,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Confusion matrix for my QDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     true\n",
       "pred    No  Yes\n",
       "  No  1916   52\n",
       "  Yes    8   24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusionmatrix_qda <- table(data.frame(pred=qda.pred$class, true=Default[-train,]$default))\n",
    "confusionmatrix_qda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background:yellow'> calculate precision, recall and F1-score </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.75"
      ],
      "text/latex": [
       "0.75"
      ],
      "text/markdown": [
       "0.75"
      ],
      "text/plain": [
       "[1] 0.75"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision_qda <- confusionmatrix_qda[2,2] / sum(table(data.frame(qda.pred$class, Default[-train,]$default))[2,])\n",
    "precision_qda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.315789473684211"
      ],
      "text/latex": [
       "0.315789473684211"
      ],
      "text/markdown": [
       "0.315789473684211"
      ],
      "text/plain": [
       "[1] 0.3157895"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recall_qda <- confusionmatrix_qda[2,2] / sum(table(data.frame(qda.pred$class, Default[-train,]$default))[,2])\n",
    "recall_qda \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.444444444444444"
      ],
      "text/latex": [
       "0.444444444444444"
      ],
      "text/markdown": [
       "0.444444444444444"
      ],
      "text/plain": [
       "[1] 0.4444444"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F1score_qda <- 2*precision_qda*recall_qda/(precision_qda+recall_qda)\n",
    "F1score_qda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "-Non parametic method : no assumptions are made about the shape of the decision boundary. <br>\n",
    "KNN takes a completely different approach\n",
    "from the classifiers seen in this chapter. In order to make a prediction for\n",
    "an observation X = x, the K training observations that are closest to x are\n",
    "identified. Then X is assigned to the class to which the plurality of these\n",
    "observations belong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A matrix containing the predictors associated with the training data, labeled ***train_knn*** below.<br>\n",
    "2. A matrix containing the predictors associated with the data for which\n",
    "we wish to make predictions, labeled ***test_knn*** below.<br>\n",
    "3. A vector containing the class labels for the training observations,\n",
    "labeled ***train.default*** below.<br>\n",
    "4. A value for K, the number of nearest neighbors to be used by the\n",
    "classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_knn = cbind(Default$income, Default$balance, Default$student)[train,]\n",
    "test_knn = cbind(Default$income, Default$balance, Default$student)[-train,]\n",
    "train.default = Default$default[train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-knn() forms predictions using a single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(3)\n",
    "knn.pred = knn(train_knn, test_knn, train.default, k = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background:yellow'> discuss the relevance of predictors </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Logistic Regression, KNN does not tell us numeric explanations on which predictors are important because it is non-parametic method; we don't get a table of\n",
    "coefficients as in summary(glm.fit). <br>\n",
    "So This part is also out of my current knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background:yellow'> create a confusion matrix </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     true\n",
       "pred    No  Yes\n",
       "  No  1907   60\n",
       "  Yes   17   16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusionmatrix_knn <- table(data.frame(pred=knn.pred, true=Default[-train,]$default))\n",
    "confusionmatrix_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background:yellow'> calculate precision, recall and F1-score </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.484848484848485"
      ],
      "text/latex": [
       "0.484848484848485"
      ],
      "text/markdown": [
       "0.484848484848485"
      ],
      "text/plain": [
       "[1] 0.4848485"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision_knn <- confusionmatrix_knn[2,2] / sum(confusionmatrix_knn[2,])\n",
    "precision_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.210526315789474"
      ],
      "text/latex": [
       "0.210526315789474"
      ],
      "text/markdown": [
       "0.210526315789474"
      ],
      "text/plain": [
       "[1] 0.2105263"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recall_knn <- confusionmatrix_knn[2,2] / sum(confusionmatrix_knn[,2])\n",
    "recall_knn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.293577981651376"
      ],
      "text/latex": [
       "0.293577981651376"
      ],
      "text/markdown": [
       "0.293577981651376"
      ],
      "text/plain": [
       "[1] 0.293578"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F1score_knn <- 2*precision_knn*recall_knn/(precision_knn+recall_knn)\n",
    "F1score_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background:yellow'> discuss differences between the approaches <span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ***Logistic regression*** if we were to use linear regression to predict a binary qualitative response some estimates could lie outside of the 0 to 1 interval, making them difficult to interpret as probabilities. <br>\n",
    "However, we also use ***LDA(Linear Discriminant Analysis)*** for these reasons we've learned; 1. The paramater estimates for logistic regression tend to be unstable when the classes are well separated. LDA does not have this problem. 2. If the predictors have an approximately normal distribution for each class, and n is small, than LDA is more stable than logisitic regression. 3. LDA is more frequently used when there are more than two response classes.<br> ***QDA(Quadratic Discriminant Analysis)*** is similar with LDA in that it results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes theorem in order to perform prediction. However, unlike LDA, ***QDA*** assumes that each class has its own covariance matrix. That is, it assumes that an observation from the kth class is of the form X ∼ N(μk, Σk), where Σk is a covariance matrix for the kth class. <br> As I saw in our mandatory text book, ***KNN(K-Nearest Neighbors)*** attempts to estimate the conditional distribution of Y given X, and then classify a given observation to the class with highest estimated probability. <br><br>\n",
    "But there is no absolute ranking between them because they sometimes show strong performance but sometimes don't depending on how things are going. <br><br>For example, LDA is typically better than QDA with relatively few training observations because in this case variance of the classifier could be a major concern. Also, LDA is usually better than Logistic with assuming that the observations are drawn\n",
    "from a Gaussian distribution with a common covariance matrix in each\n",
    "class. But when this assumption is not true, Logistic could outperform LDA. Plus, Logistic and LDA who are seemingly very similar differ in that, in log odds, beta0 and beta1 in Logistic are estimated\n",
    "using maximum likelihood, whereas c0 and c1 in LDA are computed using the estimated\n",
    "mean and variance from a normal distribution,,,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='background:yellow'> take the best approach and compare it with its own using a 10-fold cross-validation approach </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Considering the F1 score of each four approach, ***Logistic regression*** outperformed the others in our case. (Logistic Regression : 0.46 > QDA : 0.44 > LDA : 0.41 > KNN : 0.3). Given this ranking, we could infer that our training observations were quite small, and that the observations are not drawn\n",
    "from a Gaussian distribution with a common covariance matrix in each\n",
    "class ***(Just inferrence from the ranking, never affects my objective output)***. Anyway, I'll conduct a 10-fold CV on my glm.fit model. <br> <br>\n",
    "Cross-validation is a technique to evaluate predictive models by partitioning the original sample into a training set to train the model, and a test set to evaluate it. And ***10-fold CV*** involves randomly dividing the set of observations into 10 folds of approximately\n",
    "equal size. The first fold is treated as a validation set, and the method\n",
    "is fit on the remaining 9 folds. The mean squared error, MSE1, is\n",
    "then computed on the observations in the held-out fold. This procedure is\n",
    "repeated 10 times; each time, a different group of observations is treated\n",
    "as a validation set. This process results in 10 estimates of the test error,\n",
    "MSE1, MSE2,..., MSE10. The 10-fold CV estimate is computed by averaging\n",
    "these values. <br><br> Here, with cv.glm() function, I conducted 10-fold CV aganinst our logistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.0213954440612549"
      ],
      "text/latex": [
       "0.0213954440612549"
      ],
      "text/markdown": [
       "0.0213954440612549"
      ],
      "text/plain": [
       "[1] 0.02139544"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm.fit <- glm(default ~ income + balance + student, data = Default, family = \"binomial\")\n",
    "cv.err <- cv.glm(Default, glm.fit, K=10)\n",
    "cv.err$delta[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Training error, MSE obtained by the gap between the estimated value and true value, is calculated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.021298141769124"
      ],
      "text/latex": [
       "0.021298141769124"
      ],
      "text/markdown": [
       "0.021298141769124"
      ],
      "text/plain": [
       "[1] 0.02129814"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean((predict(glm.fit, new_data = Default[-train,], type = 'response') - (as.numeric(Default$default) - 1))^2 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "-Predicted MSE calculated from 10-fold CV has a tendency not to overestimate true test MSE unlike other general validation approaches.\n",
    "<br> In my case, <u> MSE I calculated manually for certain test set (A)</u> is slightly lower than <u>10-fold CV's MSE (B)</u> but if I sample the train set agian and again (without set.seed(something)) (A) could become much higher or much lower whearas (B) is absolutely stable - without any change. Anyway, as this formula shows : MSE(Mean Squared Error) = sum((y-y hat)^2)/n, MSE gets lower as the model's predictions get closer to the answer(true). <br>\n",
    "\n",
    "In a conclusion, MSEs, especially our 10-fold CV's MSE is very closer to 0, which means my model is decent no matter how the samples were drawn. \n",
    "\n",
    "Plus, through comparing how our model with changing train set predicted with MSE of 10-fold cross-validation, we could assure whether this model is biased or not. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
